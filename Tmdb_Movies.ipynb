{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Exploratory analysis and prediction on the \"TMDB 5000 Movie Dataset\" dataset**\n",
        "\n",
        "***Authors: Bava Flavio 4836427 , Ciarlo Francesco 4640121, Oldrini Edoardo 4055097***\n",
        "\n",
        "The following data analysis aims to study an approach for the production of a movie.<br><br>\n",
        "This file is divided like so:\n",
        "* Dataset checking and preparation\n",
        "* Initial exploration of the dataset\n",
        "* Proposal predictive models based on previous observations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Importing libraries and dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953456673
        }
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "import matplotlib.pyplot as plt  \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "from ast import literal_eval\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import warnings \n",
        "\n",
        "#initial settings\n",
        "warnings.filterwarnings('ignore') \n",
        "pd.set_option('display.max_columns',10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953457307
        }
      },
      "outputs": [],
      "source": [
        "Movies = pd.read_csv('input/tmdb_5000_movies.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Exploratory analysis of the dataset**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First of all, we intend to have an overall idea of the available dataset, in particular the dimensions of the dataset and the structure of the entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953457929
        }
      },
      "outputs": [],
      "source": [
        "print(\"Dataset has {} rows and {} columns\".format(Movies.shape[0],Movies.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953458602
        }
      },
      "outputs": [],
      "source": [
        "Movies.head(2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Columns like homepage, spoken_languages and title are usless or redondant, hence we proceed to drop them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953459461
        }
      },
      "outputs": [],
      "source": [
        "Movies.drop(['homepage','spoken_languages','title'],inplace=True,axis='columns')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We check if the types of datas are coherent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953460154
        }
      },
      "outputs": [],
      "source": [
        "Movies.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data types are coherent with the information they represent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We check if any null values are in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953460800
        }
      },
      "outputs": [],
      "source": [
        "Movies.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The column tagline has a huge number of null values, we will manage them when we'll work on this feature."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some columns are in Json format, hence we proceed to convert them to lists"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definition of auxiliary funcitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953461495
        }
      },
      "outputs": [],
      "source": [
        "def get_name(x):\n",
        "    if isinstance(x, list):\n",
        "        names = [i['name'] for i in x]\n",
        "        return names\n",
        "    return []\n",
        "\n",
        "def get_ISO(x):\n",
        "    if isinstance(x, list):\n",
        "        isos = [i['iso_3166_1'] for i in x]\n",
        "        return isos\n",
        "    return []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953462452
        }
      },
      "outputs": [],
      "source": [
        "feat_to_manage = ['genres','keywords','production_countries','production_companies']\n",
        "for f in feat_to_manage:\n",
        "    Movies[f] = Movies[f].apply(literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953462886
        }
      },
      "outputs": [],
      "source": [
        "#Turn genres into list\n",
        "Movies['genres'] = Movies['genres'].apply(get_name)\n",
        "#Turn prod_countries into list\n",
        "Movies['production_countries'] = Movies['production_countries'].apply(get_ISO)\n",
        "#Turn prod_companies into list\n",
        "Movies['production_companies'] = Movies['production_companies'].apply(get_name)\n",
        "#Turn keywords into list\n",
        "Movies['keywords'] = Movies['keywords'].apply(get_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some movies are in post producion or are still just rumored but we want to work only on released movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953463505
        }
      },
      "outputs": [],
      "source": [
        "Movies = Movies.query('status == \"Released\"')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dataset Analysis**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Budget"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's have a first look to the budget feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953464190
        }
      },
      "outputs": [],
      "source": [
        "Movies['budget'].describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The minimum value of the budget feature is 0. We must discard movies with a non acceptable budget, hence we keep only movie budgets with greater than 10 k, any value < 10 k is interpreted as wrong hence put o nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953464938
        }
      },
      "outputs": [],
      "source": [
        "for row in Movies.index:\n",
        "    if Movies.loc[row,'budget'] < 10000:\n",
        "        Movies.loc[row,'budget'] = np.nan\n",
        "        \n",
        "Movies['budget'].describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now the budget are acceotable, we divide movies in three classes by budget:\n",
        "- Low: 1.000000e+04 <= x <= 8.975000e+06 (class 1) \n",
        "- Medium: 8.975000e+06 < x <= 5.000000e+07 (class 2)\n",
        "- High: 5.000000e+07 < x <= 3.800000e+08 (class 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953465594
        }
      },
      "outputs": [],
      "source": [
        "bins = [1.000000e+04, 8.975000e+06, 5.000000e+07, 3.800000e+08]\n",
        "labels=[1,2,3]\n",
        "Movies['budget_class'] = pd.cut(Movies['budget'],bins=bins,labels=labels)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's print the count of movies for each budget class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953466325
        }
      },
      "outputs": [],
      "source": [
        "ax = Movies['budget_class'].value_counts().sort_values(ascending=True).plot(kind='bar')\n",
        "ax.set_xlabel(\"Budget Class\")\n",
        "ax.set_ylabel(\"Quantity\")\n",
        "plt.xticks(rotation=\"horizontal\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph is coherent with the divisions we made"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's check the distribution of budgets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953467016
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.distplot(Movies['budget'])\n",
        "sns.set(rc={'figure.figsize':(12,6)})\n",
        "plt.suptitle('Budget distribution')\n",
        "plt.show()\n",
        "\n",
        "print(\"Budget skewness: \",Movies['budget'].skew())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution is skewed, this could be a problem for the machine learning algorithm, hance we try to adjust the skewness of the distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953467718
        }
      },
      "outputs": [],
      "source": [
        "pt = PowerTransformer(method='box-cox',standardize=False)\n",
        "Movies['transf_budget'] = pt.fit_transform(Movies[['budget']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953468634
        }
      },
      "outputs": [],
      "source": [
        "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\n",
        "sns.distplot(Movies['budget'],ax=ax1)\n",
        "sns.distplot(Movies['transf_budget'],ax=ax2)\n",
        "fig.suptitle(\"Comparison between budget and transf_budget\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953469102
        }
      },
      "outputs": [],
      "source": [
        "print(\"Skewness before log : {} and Skewness after log : {}\".format(Movies['budget'].skew(),Movies['transf_budget'].skew()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Revenues ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953469764
        }
      },
      "outputs": [],
      "source": [
        "print('Movies with 0$ revenues: ',Movies[Movies['revenue'] == 0].shape[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also set these to Nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953470539
        }
      },
      "outputs": [],
      "source": [
        "for row in Movies.index:\n",
        "    if (Movies.loc[row, 'revenue'] == 0):\n",
        "        Movies.loc[row, 'revenue'] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953471329
        }
      },
      "outputs": [],
      "source": [
        "Movies['revenue'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953472177
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer(method='box-cox',standardize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953473221
        }
      },
      "outputs": [],
      "source": [
        "Movies['transf_revenue'] = pt.fit_transform(Movies[['revenue']])\n",
        "fig , (ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\n",
        "\n",
        "sns.distplot(Movies['revenue'],ax=ax1)\n",
        "sns.distplot(Movies['transf_revenue'],ax=ax2)\n",
        "fig.suptitle(\"Comparison between revenue and transf_revenue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953526582
        }
      },
      "outputs": [],
      "source": [
        "print(\"Skewness before log : {} and Skewness after log : {}\".format(Movies['revenue'].skew(),Movies['transf_revenue'].skew()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score ##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to eliminate those films that have a low percentage of vote_count(votes given), because it would create imbalances,\n",
        "given that a film voted 8, but by 5 people, is not reliable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953536341
        }
      },
      "outputs": [],
      "source": [
        "#TODO: potremmo anche fare questo lavoro sul dataset di partenza aggiungendo una colonna, metÃ  avranno nan\n",
        "C = Movies['vote_average'].mean()\n",
        "C\n",
        "m = Movies['vote_count'].quantile(0.5)\n",
        "\n",
        "for i in Movies.index:\n",
        "    if Movies.loc[i,'vote_count'] <= m:\n",
        "        Movies.loc[i,'vote_count'] = np.nan\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    \n",
        "q_movies = Movies[['id','vote_count','vote_average']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider it important that the average vote is weighted with the number of votes that generate it, to do this we use the formula recommended by the IMBD site and remove the two columns vote_average and vote_count to merge them into one that combines them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953536849
        }
      },
      "outputs": [],
      "source": [
        "#Weighted Rating\n",
        "def weighted_rating(x, m=m, C=C):\n",
        "    v = x['vote_count']\n",
        "    R = x['vote_average']\n",
        "    # Calculation based on the IMDB formula\n",
        "    return (v/(v+m) * R) + (m/(m+v) * C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953537744
        }
      },
      "outputs": [],
      "source": [
        "# Define a new feature 'score' and calculate its value with `weighted_rating()`\n",
        "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)\n",
        "\n",
        "q_movies.drop(['vote_average','vote_count'],inplace=True,axis='columns')\n",
        "\n",
        "#Dataframe merge, now Movies also has Score column\"\n",
        "Movies = pd.merge(Movies,q_movies,on='id',how='inner')\n",
        "\n",
        "Movies.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have eliminated some films (in q_movies), those with too few votes to be taken into consideration with regard to the received vote and then we have combined the two dataframes, so now Movies also has the score column"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profits ##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We measure the revenues against the budget through a function, and add a new column called 'profit_perc' containing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953538585
        }
      },
      "outputs": [],
      "source": [
        "def calculate_profit_perc(x):\n",
        "    if (x.revenue>0) and (x.budget>0):\n",
        "        return ((x.revenue-x.budget)/x.budget)*100\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953541283
        }
      },
      "outputs": [],
      "source": [
        "Movies = Movies.assign(profit_perc = lambda x: x.budget)\n",
        "for row in Movies.index:\n",
        "    Movies.loc[row,'profit_perc'] =  calculate_profit_perc(Movies.loc[row])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a function which, given a genre, calculates the average of the profits of the films that contain it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953541779
        }
      },
      "outputs": [],
      "source": [
        "temp = Movies\n",
        "temp.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953542266
        }
      },
      "outputs": [],
      "source": [
        "#How many movies are there by genre with profit other than Nan\n",
        "def films_per_genres(genre):\n",
        "    count = 0\n",
        "    for row in temp.index:\n",
        "        if (genre in temp.loc[row, 'genres'] and (temp.loc[row,'profit_perc'] != np.nan)):\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "\n",
        "#Profits by genre\n",
        "def genre_average_profits(genre):\n",
        "    sum = 0\n",
        "    count = 0\n",
        "    for row in temp.index:\n",
        "        if (genre in temp.loc[row, 'genres'] and (temp.loc[row,'profit_perc'] != np.nan)):\n",
        "            sum += temp.loc[row, 'profit_perc']\n",
        "            count+=1\n",
        "    return sum/count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953542909
        }
      },
      "outputs": [],
      "source": [
        "genres=[]\n",
        "for row in temp.index:\n",
        "    _gen = temp.loc[row,'genres']\n",
        "    for g in _gen:\n",
        "        if g not in genres:\n",
        "            genres.append(g)\n",
        "\n",
        "profits=[]\n",
        "for g in genres:\n",
        "    profits.append(genre_average_profits(g))\n",
        "            \n",
        "print(\"Profits for each genre:\")\n",
        "for i in range(0,len(genres)):\n",
        "    print('\\t',genres[i], \" has a mean profit of \", profits[i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe how some films have a very high profit, this is given by the fact that for example Horror films have earned a lot and there are few within the dataset, so let's see the cardinalities of each genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953543393
        }
      },
      "outputs": [],
      "source": [
        "films = []\n",
        "for el in genres:\n",
        "    films.append(films_per_genres(el))\n",
        "    \n",
        "print(\"Genres in the dataframe are:\")\n",
        "for genre,film in zip(genres,films):\n",
        "    print(genre,film)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Documentaries, for example, are very few compared to the total of films and have a percentage profit of around 6119, therefore, for the reason mentioned above, they greatly unbalance the accounts. We can afford to exclude them, since there are only 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953543891
        }
      },
      "outputs": [],
      "source": [
        "genres.remove(genres[-1])\n",
        "profits.remove(profits[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953544413
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.bar(genres,profits)\n",
        "fig.set_figwidth(27)\n",
        "fig.set_figheight(13)\n",
        "plt.xticks(rotation=45,fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.xlabel(\"Genres\", fontsize=20)\n",
        "plt.ylabel(\"Profits\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953545428
        }
      },
      "outputs": [],
      "source": [
        "log_profit = np.log1p(profits)\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(genres,log_profit)\n",
        "fig.set_figwidth(27)\n",
        "fig.set_figheight(13)\n",
        "plt.xticks(rotation=45,fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.xlabel(\"Genres\", fontsize=20)\n",
        "plt.ylabel(\"Log Profits\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords ##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what are the most frequent words present among the keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676301705732
        }
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "\n",
        "\n",
        "plt.subplots(figsize=(12,12))\n",
        "stop_words=set(stopwords.words('english'))\n",
        "stop_words.update(',',';','!','?','.','(',')','$','#','+',':','...',' ','')\n",
        "\n",
        "Movies['keywords'].dropna(inplace=True)\n",
        "keywords_converted = Movies['keywords'].astype(str)\n",
        "words=keywords_converted.apply(nltk.word_tokenize)\n",
        "word=[]\n",
        "for i in words:\n",
        "    word.extend(i)\n",
        "word=pd.Series(word)\n",
        "word=([i for i in word.str.lower() if i not in stop_words])\n",
        "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=STOPWORDS, max_font_size= 60,width=1000,height=600)\n",
        "wc.generate(\" \".join(word))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,10)\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Release Date ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953546526
        }
      },
      "outputs": [],
      "source": [
        "#Convert the Relase date to datetime format\n",
        "temp['release_date'] = pd.to_datetime(temp['release_date'])\n",
        "temp['release_year'] = temp['release_date'].dt.year\n",
        "temp['release_month'] = temp['release_date'].dt.month\n",
        "temp['release_day'] = temp['release_date'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953547013
        }
      },
      "outputs": [],
      "source": [
        "temp['decades'] = temp['release_date'].apply(lambda x : (x.year // 10)*10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantity of films released during months of the year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953547771
        }
      },
      "outputs": [],
      "source": [
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "sns.countplot(x='release_month',data=temp)\n",
        "plt.suptitle(\"Films released every month\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953548563
        }
      },
      "outputs": [],
      "source": [
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "sns.countplot(x='release_day',data=temp)\n",
        "plt.suptitle(\"Films released every day\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953549868
        }
      },
      "outputs": [],
      "source": [
        "ax = sns.scatterplot(x=\"release_date\",y=\"revenue\",data=temp,hue=\"budget_class\")\n",
        "sns.set(rc = {'figure.figsize':(6,6)})\n",
        "ax.set_title(\"Revenue during year\")\n",
        "ax.set_xlabel(\"Release Year\")\n",
        "ax.set_ylabel(\"Revenue\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph above shows how revenues have increased over time, as have budgets invested"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953563765
        }
      },
      "outputs": [],
      "source": [
        "d2 = temp.groupby(['release_month'])['revenue'].mean()\n",
        "data = [go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y')]\n",
        "layout = go.Layout(dict(title = \"Average revenue per month\",\n",
        "                  xaxis = dict(title = 'Month'),\n",
        "                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n",
        "                  ),legend=dict(\n",
        "                orientation=\"v\"))\n",
        "py.iplot(dict(data=data, layout=layout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953564551
        }
      },
      "outputs": [],
      "source": [
        "d2 = temp.groupby(['release_day'])['revenue'].mean()\n",
        "data = [go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y')]\n",
        "layout = go.Layout(dict(title = \"Average revenue per Day\",\n",
        "                  xaxis = dict(title = 'Day'),\n",
        "                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n",
        "                  ),legend=dict(\n",
        "                orientation=\"v\"))\n",
        "py.iplot(dict(data=data, layout=layout))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Popularity ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953565206
        }
      },
      "outputs": [],
      "source": [
        "temp['log_popularity'] = np.log(temp['popularity'])\n",
        "\n",
        "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\n",
        "\n",
        "sns.distplot(temp['popularity'],ax=ax1)\n",
        "sns.distplot(temp['log_popularity'],ax=ax2)\n",
        "\n",
        "fig.suptitle(\"Comparison between popularity and log_popularity skewness\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953565841
        }
      },
      "outputs": [],
      "source": [
        "print(\"Skewness before log : {} and Skewness after log : {}\".format(temp['popularity'].skew(),temp['log_popularity'].skew()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953566937
        }
      },
      "outputs": [],
      "source": [
        "sns.set(rc = {'figure.figsize':(12,6)})\n",
        "ax = sns.scatterplot(x='popularity',y='revenue',data=temp,hue=\"budget_class\")\n",
        "ax.set_title(\"Popularity and revenues divided by budget_class\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cast & Directors #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953567612
        }
      },
      "outputs": [],
      "source": [
        "temp.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953568278
        }
      },
      "outputs": [],
      "source": [
        "cast = pd.read_csv(\"input/tmdb_5000_credits.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953568841
        }
      },
      "outputs": [],
      "source": [
        "feat_to_manage = ['cast','crew']\n",
        "for f in feat_to_manage:\n",
        "    cast[f] = cast[f].apply(literal_eval)\n",
        "\n",
        "#Two functions that convert directors and actors from json to list-str\n",
        "def get_director(x):\n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan\n",
        "\n",
        "def get_actors(x):\n",
        "    if isinstance(x, list):\n",
        "        names = [i['name'] for i in x]\n",
        "        \n",
        "        return names\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953569461
        }
      },
      "outputs": [],
      "source": [
        "#Create two new column correctly formatted\n",
        "cast['director'] = cast['crew'].apply(get_director)\n",
        "cast['actors'] = cast['cast'].apply(get_actors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953570038
        }
      },
      "outputs": [],
      "source": [
        "#Drop old columns\n",
        "cast.drop('cast',inplace=True,axis=1)\n",
        "cast.drop('crew',inplace=True,axis=1)\n",
        "cast.drop('title',inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953571184
        }
      },
      "outputs": [],
      "source": [
        "#rename Movie_id to id, preparing for the merge\n",
        "cast = cast.rename(columns={'movie_id': 'id'})\n",
        "\n",
        "#Merge two dataframe Movies,cast\n",
        "full_df = pd.merge(temp,cast,on=\"id\",how=\"inner\")\n",
        "recommend_df = full_df.copy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actors ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953572375
        }
      },
      "outputs": [],
      "source": [
        "actors=[]\n",
        "\n",
        "\n",
        "for i in full_df['actors']:\n",
        "    actors.extend(i)\n",
        "\n",
        "actors = list(filter(None, actors))\n",
        "\n",
        "\n",
        "plt.subplots(figsize=(12,10))\n",
        "ax=pd.Series(actors).value_counts()[:15].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette('inferno_r',40))\n",
        "for i, v in enumerate(pd.Series(actors).value_counts()[:15].sort_values(ascending=True).values): \n",
        "    ax.text(.8, i, v,fontsize=10,color='black',weight='bold')\n",
        "\n",
        "plt.title('Actors with highest appearance')\n",
        "ax.patches[14].set_facecolor('r')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Directors ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953573591
        }
      },
      "outputs": [],
      "source": [
        "directors=[]\n",
        "\n",
        "\n",
        "for i in full_df['director']:\n",
        "    directors.append(i)\n",
        "\n",
        "directors = list(filter(None, directors))\n",
        "\n",
        "\n",
        "plt.subplots(figsize=(12,10))\n",
        "ax=pd.Series(directors).value_counts()[:14].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette('inferno_r',40))\n",
        "for i, v in enumerate(pd.Series(directors).value_counts()[:14].sort_values(ascending=True).values): \n",
        "    ax.text(.8, i, v,fontsize=10,color='black',weight='bold')\n",
        "\n",
        "plt.title('Directors with highest appearance')\n",
        "ax.patches[13].set_facecolor('r')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We calculate the average Score for the most present directors (>= 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953574234
        }
      },
      "outputs": [],
      "source": [
        "#Filter the directors with made films >= 10,then calculate the mean scores\n",
        "director_group = full_df.groupby('director').filter(lambda x : len(x) >= 10)\n",
        "mean_scores = director_group.groupby('director')['score'].mean().sort_values(ascending=False).reset_index(name=\"score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953574882
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.bar(mean_scores['director'],mean_scores['score'],color='navy')\n",
        "fig.set_figwidth(20)\n",
        "fig.set_figheight(13)\n",
        "plt.xticks(rotation=45,fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.xlabel(\"Directors\", fontsize=20)\n",
        "plt.ylabel(\"Scores\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictions #"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Revenues ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953576155
        }
      },
      "outputs": [],
      "source": [
        "new_temp=pd.merge(temp,cast,on=\"id\",how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953578496
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "new_temp['production_companies'] = new_temp['production_companies'].apply(lambda x : x[0] if len(x) > 0 else None)\n",
        "new_temp['production_companies'] = new_temp['production_companies'].apply(lambda x:re.sub('[^A-Za-z0-9_]+', '', str(x)))\n",
        "\n",
        "new_temp['actors'] = new_temp['actors'].apply(lambda x : x[0:3] if len(x) > 0 else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953579507
        }
      },
      "outputs": [],
      "source": [
        "#genres\n",
        "df=pd.DataFrame( {'genres': new_temp['genres']})\n",
        "df= pd.get_dummies(df.genres.apply(pd.Series).stack()).sum(level=0)\n",
        "new_temp = pd.concat([new_temp,df],axis = 1)\n",
        "\n",
        "#production companies\n",
        "df=pd.DataFrame( {'production_companies': new_temp['production_companies']})\n",
        "df= pd.get_dummies(df.production_companies.apply(pd.Series).stack()).sum(level=0)\n",
        "new_temp = pd.concat([new_temp,df],axis = 1)\n",
        "\n",
        "#production countries\n",
        "df=pd.DataFrame( {'production_countries': new_temp['production_countries']})\n",
        "df= pd.get_dummies(df.production_countries.apply(pd.Series).stack()).sum(level=0)\n",
        "new_temp = pd.concat([new_temp,df],axis = 1)\n",
        "\n",
        "\n",
        "#actors\n",
        "df=pd.DataFrame( {'actors': new_temp['actors']})\n",
        "df= pd.get_dummies(df.actors.apply(pd.Series).stack()).sum(level=0)\n",
        "new_temp = pd.concat([new_temp,df],axis = 1)\n",
        "\n",
        "#director\n",
        "df=pd.DataFrame( {'director': new_temp['director']})\n",
        "df= pd.get_dummies(df.director.apply(pd.Series).stack()).sum(level=0)\n",
        "new_temp = pd.concat([new_temp,df],axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953580108
        }
      },
      "outputs": [],
      "source": [
        "drop_columns=['budget','status','release_date','tagline', 'overview','vote_count','vote_average','original_title','original_language','id','revenue','profit_perc','genres', 'keywords','popularity','production_companies','production_countries','actors','director']#\n",
        "new_temp= new_temp.drop(drop_columns, axis=1)    \n",
        "new_temp = new_temp.loc[:,~new_temp.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Feature scaling \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "new_temp['transf_revenue'] = scaler.fit_transform(new_temp[['transf_revenue']])\n",
        "new_temp['transf_budget'] = scaler.fit_transform(new_temp[['transf_budget']])\n",
        "new_temp['score'] = scaler.fit_transform(new_temp[['score']])\n",
        "new_temp['runtime'] = scaler.fit_transform(new_temp[['runtime']])\n",
        "new_temp['log_popularity'] = scaler.fit_transform(new_temp[['log_popularity']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953580690
        }
      },
      "outputs": [],
      "source": [
        "#Rename columns to filter special characters\n",
        "new_temp = new_temp.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', str(x)))\n",
        "\n",
        "#Drop Walt Disney because it's duplicate\n",
        "new_temp.drop('WaltDisney',inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953581219
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953667897
        }
      },
      "outputs": [],
      "source": [
        "# Formating for modeling\n",
        "new_temp=new_temp.dropna()\n",
        "y = new_temp['transf_revenue']\n",
        "X = new_temp.drop(['transf_revenue'], axis=1)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hyperparams Tuning**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "   \n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    \n",
        "    param = {\n",
        "        \"objective\": \"regression\",\n",
        "        \"metric\": \"rmse\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"max_depth\" : trial.suggest_int(\"max_depth\",5, 9),\n",
        "        \"learning_rate\" : trial.suggest_float(\"learning_rate\",0.01,0.03),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
        "        \"max_bin\" : trial.suggest_int(\"max_bin\",5,50),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "        \"min_data_in_leaf\" : trial.suggest_int(\"min_data_in_leaf\",5,50),\n",
        "        \"n_estimators\" : trial.suggest_int(\"n_estimators\",500, 2000),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "        \n",
        "    }\n",
        "\n",
        "    gbm = lgb.train(param, dtrain)\n",
        "    preds = gbm.predict(X_valid)\n",
        "    pred_labels = np.rint(preds)\n",
        "    accuracy = r2_score(y_valid,preds)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt_params = {}\n",
        "for key, value in study.best_trial.params.items():\n",
        "    opt_params[key] = [value]\n",
        "\n",
        "opt_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GridSearchCV ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_params = {\n",
        "         'n_estimators' : [1000],\n",
        "         'num_leaves': [5,10],\n",
        "         'objective': ['regression'],\n",
        "         'max_depth': [9,7,10],\n",
        "         'max_bins' : [10,20],\n",
        "         'learning_rate': [0.01],\n",
        "         \"boosting\": [\"gbdt\"],\n",
        "         \"feature_fraction\": [0.9],\n",
        "         \"bagging_fraction\": [0.9],\n",
        "         \"metric\": ['r2'],\n",
        "         \"lambda_l1\": [0.2],\n",
        "         \"verbosity\" : [-1]\n",
        "\n",
        "        }\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor()\n",
        "\n",
        "gs = GridSearchCV(lgb_model,param_grid=opt_params,verbose=1,scoring=\"r2\",refit=\"r2\",cv=7,n_jobs=-1)\n",
        "\n",
        "gs.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gs.best_score_\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LGBRegressor ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586494
        }
      },
      "outputs": [],
      "source": [
        "lgb_model = lgb.LGBMRegressor(**gs.best_params_,nthread = 4,n_jobs = -1)\n",
        "lgb_model.fit(X_train, y_train, \n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
        "        verbose=1, early_stopping_rounds=100)\n",
        "\n",
        "y_pred = lgb_model.predict(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586543
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score \n",
        "\n",
        "\n",
        "def print_errors(real_value, predicted_value):\n",
        "    print('\\tMean absolute error:', mean_absolute_error(real_value, predicted_value))\n",
        "    print('\\tMean squared error', mean_squared_error(real_value, predicted_value))\n",
        "    print('\\tRMSE:', np.sqrt(mean_squared_error(real_value, predicted_value)))\n",
        "    print('\\tScore: ',r2_score(real_value,predicted_value))\n",
        "\n",
        "print_errors(y_valid, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586589
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Real Values': y_valid, 'Predicted Values': y_pred})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586633
        }
      },
      "outputs": [],
      "source": [
        "lgb.plot_importance(lgb_model,max_num_features=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586728
        }
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_valid, y_pred)\n",
        "tmp = [min(np.concatenate((y_train,y_valid))),\n",
        "       max(np.concatenate((y_train,y_valid)))]\n",
        "plt.plot(tmp,tmp,'r')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score ##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert categorical features to a list of binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586780
        }
      },
      "outputs": [],
      "source": [
        "def binary(genre_list):\n",
        "    binaryList = []\n",
        "    \n",
        "    for genre in genres:\n",
        "        if genre in genre_list:\n",
        "            binaryList.append(1)\n",
        "        else:\n",
        "            binaryList.append(0)\n",
        "    \n",
        "    return binaryList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586826
        }
      },
      "outputs": [],
      "source": [
        "full_df['genres_bin'] = full_df['genres'].apply(lambda x: binary(x))\n",
        "full_df['genres_bin'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586877
        }
      },
      "outputs": [],
      "source": [
        "for i,j in zip(full_df['actors'],full_df.index):\n",
        "    list2=[]\n",
        "    list2=i[:4]\n",
        "    full_df.loc[j,'actors']=str(list2)\n",
        "full_df['actors']=full_df['actors'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n",
        "full_df['actors']=full_df['actors'].str.split(',')\n",
        "for i,j in zip(full_df['actors'],full_df.index):\n",
        "    list2=[]\n",
        "    list2=i\n",
        "    list2.sort()\n",
        "    full_df.loc[j,'actors']=str(list2)\n",
        "full_df['actors']=full_df['actors'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n",
        "full_df['actors']=full_df['actors'].str.split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586928
        }
      },
      "outputs": [],
      "source": [
        "full_df['actors'].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953586972
        }
      },
      "outputs": [],
      "source": [
        "actor_list=[]\n",
        "for row in full_df.index:\n",
        "    _actors = full_df.loc[row,'actors']\n",
        "    for g in _actors:\n",
        "        if g not in actor_list:\n",
        "            actor_list.append(g)\n",
        "\n",
        "\n",
        "def binary(cast_list):\n",
        "    binaryList = []\n",
        "    \n",
        "    for genre in actor_list:\n",
        "        if genre in cast_list:\n",
        "            binaryList.append(1)\n",
        "        else:\n",
        "            binaryList.append(0)\n",
        "    \n",
        "    return binaryList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587022
        }
      },
      "outputs": [],
      "source": [
        "full_df['actors_bin'] = full_df['actors'].apply(lambda x: binary(x))\n",
        "full_df['actors_bin'].head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587069
        }
      },
      "outputs": [],
      "source": [
        "for i,j in zip(full_df['director'],full_df.index):\n",
        "    list2=[]\n",
        "    list2=i[:4]\n",
        "    full_df.loc[j,'director']=str(list2)\n",
        "full_df['director']=full_df['director'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n",
        "full_df['director']=full_df['director'].str.split(',')\n",
        "for i,j in zip(full_df['director'],full_df.index):\n",
        "    list2=[]\n",
        "    list2=i\n",
        "    list2.sort()\n",
        "    full_df.loc[j,'director']=str(list2)\n",
        "full_df['director']=full_df['director'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n",
        "full_df['director']=full_df['director'].str.split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587117
        }
      },
      "outputs": [],
      "source": [
        "director_list=[]\n",
        "for row in full_df.index:\n",
        "    _actors = full_df.loc[row,'director']\n",
        "    for g in _actors:\n",
        "        if g not in director_list:\n",
        "            director_list.append(g)\n",
        "\n",
        "def binary(directors):\n",
        "    binaryList = []\n",
        "    \n",
        "    for direct in director_list:\n",
        "        if direct in directors:\n",
        "            binaryList.append(1)\n",
        "        else:\n",
        "            binaryList.append(0)\n",
        "    \n",
        "    return binaryList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587169
        }
      },
      "outputs": [],
      "source": [
        "full_df['director_bin'] = full_df['director'].apply(lambda x: binary(x))\n",
        "full_df['director_bin'].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676301713437
        }
      },
      "outputs": [],
      "source": [
        "full_df['keywords'] = full_df['keywords'].astype(str)\n",
        "\n",
        "full_df['keywords']=full_df['keywords'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'').str.replace('\"','')\n",
        "full_df['keywords']=full_df['keywords'].str.split(',')\n",
        "for i,j in zip(full_df['keywords'],full_df.index):\n",
        "    list2=[]\n",
        "    list2=i[:4]\n",
        "    full_df.loc[j,'keywords']=str(list2)\n",
        "full_df['keywords']=full_df['keywords'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n",
        "full_df['keywords']=full_df['keywords'].str.split(',')\n",
        "for i,j in zip(full_df['keywords'],full_df.index):\n",
        "    list2=[]\n",
        "    list2=i\n",
        "    list2.sort()\n",
        "    full_df.loc[j,'keywords']=str(list2)\n",
        "full_df['keywords']=full_df['keywords'].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n",
        "full_df['keywords']=full_df['keywords'].str.split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587267
        }
      },
      "outputs": [],
      "source": [
        "words_list = []\n",
        "for index, row in full_df.iterrows():\n",
        "    genres = row[\"keywords\"]\n",
        "    \n",
        "    for genre in genres:\n",
        "        if genre not in words_list:\n",
        "            words_list.append(genre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587316
        }
      },
      "outputs": [],
      "source": [
        "def binary(words):\n",
        "    binaryList = []\n",
        "    \n",
        "    for genre in words_list:\n",
        "        if genre in words:\n",
        "            binaryList.append(1)\n",
        "        else:\n",
        "            binaryList.append(0)\n",
        "    \n",
        "    return binaryList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587364
        }
      },
      "outputs": [],
      "source": [
        "full_df['words_bin'] = full_df['keywords'].apply(lambda x: binary(x))\n",
        "full_df['words_bin'].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587419
        }
      },
      "outputs": [],
      "source": [
        "full_df = full_df[(full_df['vote_average']!=0)] #removing the movies with 0 score and without drector names \n",
        "full_df = full_df[full_df['director']!='']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587468
        }
      },
      "outputs": [],
      "source": [
        "new_id = list(range(0,full_df.shape[0]))\n",
        "full_df['new_id']=new_id\n",
        "full_df=full_df[['original_title','genres','vote_average','score','genres_bin','actors_bin','new_id','director','director_bin','words_bin']]\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587515
        }
      },
      "outputs": [],
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def Similarity(movieId1, movieId2):\n",
        "    a = full_df.iloc[movieId1]\n",
        "    b = full_df.iloc[movieId2]\n",
        "    \n",
        "    genresA = a['genres_bin']\n",
        "    genresB = b['genres_bin']\n",
        "    \n",
        "    genreDistance = spatial.distance.cosine(genresA, genresB)\n",
        "    \n",
        "    scoreA = a['actors_bin']\n",
        "    scoreB = b['actors_bin']\n",
        "    scoreDistance = spatial.distance.cosine(scoreA, scoreB)\n",
        "    \n",
        "    directA = a['director_bin']\n",
        "    directB = b['director_bin']\n",
        "    directDistance = spatial.distance.cosine(directA, directB)\n",
        "    \n",
        "    return genreDistance + directDistance + scoreDistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587612
        }
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "def score_prediction(name):\n",
        "    new_movie=full_df[full_df['original_title'].str.contains(name)].iloc[0].to_frame().T\n",
        "    print('Selected Movie: ',new_movie.original_title.values[0])\n",
        "    def getNeighbors(baseMovie, K):\n",
        "        distances = []\n",
        "    \n",
        "        for index, movie in full_df.iterrows():\n",
        "            if movie['new_id'] != baseMovie['new_id'].values[0]:\n",
        "                dist = Similarity(baseMovie['new_id'].values[0], movie['new_id'])\n",
        "                distances.append((movie['new_id'], dist))\n",
        "    \n",
        "        distances.sort(key=operator.itemgetter(1))\n",
        "        neighbors = []\n",
        "    \n",
        "        for x in range(K):\n",
        "            neighbors.append(distances[x])\n",
        "        return neighbors\n",
        "\n",
        "    K = 10\n",
        "    avgScore = 0\n",
        "    neighbors = getNeighbors(new_movie, K)\n",
        "\n",
        "    \n",
        "    for neighbor in neighbors:\n",
        "        avgScore = avgScore+full_df.iloc[neighbor[0]][2]  \n",
        "    \n",
        "    print('\\n')\n",
        "    avgScore = avgScore/K\n",
        "    print('The predicted rating for %s is: %f' %(new_movie['original_title'].values[0],avgScore))\n",
        "    print('The actual rating for %s is %f' %(new_movie['original_title'].values[0],new_movie['score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_prediction('Green')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reccomendation System (Overview,Title) ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587710
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587754
        }
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "recommend_df['overview'] = recommend_df['overview'].fillna('')\n",
        "tfidf_matr = tfidf.fit_transform(recommend_df['overview'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587802
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matr,tfidf_matr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587846
        }
      },
      "outputs": [],
      "source": [
        "indices = pd.Series(recommend_df.index, index=recommend_df['original_title']).drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587894
        }
      },
      "outputs": [],
      "source": [
        "#Recommendation using cosine similarity\n",
        "def get_recommendations(title, cosine_similarity=cosine_sim):\n",
        "    # Get the index of the movie that matches the title\n",
        "    idx = indices[title]\n",
        "\n",
        "    # Get the pairwise similarity scores of all movies with that movie\n",
        "    sim_scores = list(enumerate(cosine_similarity[idx]))\n",
        "    #print(sim_scores)\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    #print(sim_scores)\n",
        "    # Get the scores of the 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the top 10 most similar movies\n",
        "    return recommend_df['original_title'].iloc[movie_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675953587942
        }
      },
      "outputs": [],
      "source": [
        "get_recommendations('Tangled')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reccomendation System (Keywords,Actors,Directors,Genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_data(x):\n",
        "    if isinstance(x, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
        "    else:\n",
        "        #Check if director exists. If not, return empty string\n",
        "        if isinstance(x, str):\n",
        "            return str.lower(x.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply clean_data function to your features.\n",
        "features = ['actors', 'keywords', 'director', 'genres']\n",
        "\n",
        "\n",
        "for feature in features:\n",
        "    recommend_df[feature] = recommend_df[feature].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We create a soup of metadata that will be feed to our Count Vectorizer\n",
        "def create_soup(x):\n",
        "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['actors']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
        "recommend_df['soup'] = recommend_df.apply(create_soup, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count = CountVectorizer(stop_words='english')\n",
        "count_matrix = count.fit_transform(recommend_df['soup'])\n",
        "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n",
        "# Reset index of our main DataFrame and construct reverse mapping as before\n",
        "recommend_df = recommend_df.reset_index()\n",
        "indices = pd.Series(recommend_df.index, index=recommend_df['original_title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_recommendations(\"Tangled\", cosine_sim2)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
